"""
Cryptocurrency News Retrieval and Processing Module

This module is used to retrieve important cryptocurrency news from reliable sources
and provides functionality for categorizing, translating, and sentiment analysis of news.
"""

import os
import logging
import requests
import json
import time
from datetime import datetime, timedelta
import pytz
from bs4 import BeautifulSoup
import trafilatura
from openai import OpenAI
from typing import List, Dict, Any, Optional

# Add access to the new cryptocurrency news API
try:
    from crypto_bot.crypto_news_api import (
        get_crypto_news_from_api,
        get_canadian_crypto_news_from_api
    )
    HAS_NEWS_API = True
    logging.info("Cryptocurrency News API module loaded successfully")
except ImportError:
    HAS_NEWS_API = False
    logging.warning("Cryptocurrency News API module not found, using legacy sources")

# CMC Markets Canada news module
try:
    from crypto_bot.cmc_canada_news import (
        get_cmc_canada_news, 
        get_cmc_canada_crypto_analysis, 
        get_combined_cmc_canada_content,
        get_ndax_blog_news,
        get_bitbuy_blog_news,
        get_newton_learn_articles,
        get_all_canadian_crypto_news
    )
except ImportError:
    logger = logging.getLogger(__name__)
    logger.warning("CMC Markets Canada news module not found. This news source will not be available.")
    
    def get_cmc_canada_news(max_items=5, use_cache=True):
        return []
    
    def get_cmc_canada_crypto_analysis(max_items=3, use_cache=True):
        return []
    
    def get_combined_cmc_canada_content(max_news=5, max_analysis=3, use_cache=True):
        return []
    
    def get_ndax_blog_news(max_items=5, use_cache=True):
        return []
    
    def get_bitbuy_blog_news(max_items=5, use_cache=True):
        return []
    
    def get_newton_learn_articles(max_items=5, use_cache=True):
        return []
    
    def get_all_canadian_crypto_news(max_per_source=3, use_cache=True):
        return []

# Setup logger
logger = logging.getLogger(__name__)

# API settings
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
CRYPTOCOMPARE_API_KEY = os.environ.get("CRYPTOCOMPARE_API_KEY")

# Set Toronto timezone
toronto_tz = pytz.timezone('America/Toronto')


def get_cryptocompare_news(limit: int = 10, lang: str = "EN") -> List[Dict[str, Any]]:
    """
    Get news from CryptoCompare API
    
    Args:
        limit (int): Number of news items needed
        lang (str): News language
        
    Returns:
        List[Dict[str, Any]]: List of retrieved news items
    """
    try:
        url = f"https://min-api.cryptocompare.com/data/v2/news/?lang={lang}&api_key={CRYPTOCOMPARE_API_KEY}&limit={limit}"
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if data.get("Response") == "Success" or data.get("Type") == 100:
                return data.get("Data", [])
            else:
                logger.warning(f"Error in CryptoCompare response: {data.get('Message')}")
                return []
        else:
            logger.warning(f"Error in CryptoCompare request: {response.status_code}")
            return []
    except Exception as e:
        logger.error(f"Error getting news from CryptoCompare: {str(e)}")
        return []


def get_coindesk_news(limit: int = 5) -> List[Dict[str, Any]]:
    """
    Get news from CoinDesk website
    
    Args:
        limit (int): Number of news items needed
        
    Returns:
        List[Dict[str, Any]]: List of retrieved news items
    """
    try:
        url = "https://www.coindesk.com/"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            for article in soup.select('article')[:limit]:
                try:
                    title_elem = article.select_one('h6') or article.select_one('h5') or article.select_one('h4')
                    link_elem = article.select_one('a')
                    
                    if title_elem and link_elem:
                        title = title_elem.text.strip()
                        link = link_elem.get('href', '')
                        if not link.startswith('http'):
                            link = f"https://www.coindesk.com{link}"
                        
                        # Get image if available
                        img_elem = article.select_one('img')
                        image_url = img_elem.get('src', '') if img_elem else ''
                        
                        articles.append({
                            "title": title,
                            "url": link,
                            "imageurl": image_url,
                            "source": "CoinDesk",
                            "published_on": int(time.time())
                        })
                except Exception as e:
                    logger.error(f"Error processing CoinDesk article: {str(e)}")
                    continue
            
            return articles
        else:
            logger.warning(f"Error in CoinDesk request: {response.status_code}")
            return []
    except Exception as e:
        logger.error(f"Error getting news from CoinDesk: {str(e)}")
        return []


def get_cointelegraph_news(limit: int = 5) -> List[Dict[str, Any]]:
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ÙˆØ¨â€ŒØ³Ø§ÛŒØª CoinTelegraph
    
    Args:
        limit (int): ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
        
    Returns:
        List[Dict[str, Any]]: Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡
    """
    try:
        url = "https://cointelegraph.com/"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # Ù…Ù‚Ø§Ù„Ø§Øª Ø§ØµÙ„ÛŒ
            for article in soup.select('.post-card')[:limit]:
                try:
                    title_elem = article.select_one('.post-card__title')
                    link_elem = article.select_one('a.post-card__link')
                    
                    if title_elem and link_elem:
                        title = title_elem.text.strip()
                        link = link_elem.get('href', '')
                        if not link.startswith('http'):
                            link = f"https://cointelegraph.com{link}"
                        
                        # Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
                        img_elem = article.select_one('img')
                        image_url = img_elem.get('src', '') if img_elem else ''
                        
                        articles.append({
                            "title": title,
                            "url": link,
                            "imageurl": image_url,
                            "source": "CoinTelegraph",
                            "published_on": int(time.time())
                        })
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø§Ù„Ù‡ CoinTelegraph: {str(e)}")
                    continue
            
            return articles
        else:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª CoinTelegraph: {response.status_code}")
            return []
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² CoinTelegraph: {str(e)}")
        return []


def extract_article_content(url: str) -> str:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø² URL
    
    Args:
        url (str): Ø¢Ø¯Ø±Ø³ Ù…Ù‚Ø§Ù„Ù‡
        
    Returns:
        str: Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ù…Ù‚Ø§Ù„Ù‡
    """
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded)
            if text:
                return text
        
        # Ø§Ú¯Ø± trafilatura Ù†ØªÙˆØ§Ù†Ø¯ Ù…Ø­ØªÙˆØ§ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ø¯ØŒ Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # Ø­Ø°Ù Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
            for elem in soup(['script', 'style', 'nav', 'footer', 'header']):
                elem.decompose()
            
            # Ú¯Ø±ÙØªÙ† Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ
            article_content = soup.select_one('article') or soup.select_one('.article-content') or soup.select_one('.content')
            if article_content:
                return article_content.get_text(separator='\n').strip()
            
            # Ø§Ú¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² Ù…ØªÙ† Ú©Ù„ ØµÙØ­Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            return soup.get_text(separator='\n').strip()
        
        return ""
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ù‚Ø§Ù„Ù‡: {str(e)}")
        return ""


def translate_news(news_items: List[Dict[str, Any]], target_language: str = "fa") -> List[Dict[str, Any]]:
    """
    ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OpenAI API
    
    Args:
        news_items (List[Dict[str, Any]]): Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø±
        target_language (str): Ø²Ø¨Ø§Ù† Ù…Ù‚ØµØ¯ (fa Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ)
        
    Returns:
        List[Dict[str, Any]]: Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ø¹Ù†Ø§ÙˆÛŒÙ† ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯Ù‡
    """
    if not OPENAI_API_KEY or not news_items:
        return news_items
    
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        translated_items = []
        
        # ØªØ±Ø¬Ù…Ù‡ Ù‡Ø± Û´ Ø®Ø¨Ø± Ø¯Ø± ÛŒÚ© Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        for i in range(0, len(news_items), 4):
            batch = news_items[i:i+4]
            titles = [item['title'] for item in batch]
            
            # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² + Ø¨Ø±Ø§ÛŒ Ú†Ø³Ø¨Ø§Ù†Ø¯Ù† Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¬Ø§ÛŒ f-string
            titles_formatted = "\n".join([f"{idx+1}. {title}" for idx, title in enumerate(titles)])
            prompt = "ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø²ÛŒØ± Ø§Ø² Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø±ÙˆØ§Ù†:\n\n" + titles_formatted + "\n\nÙ¾Ø§Ø³Ø® Ø±Ø§ ÙÙ‚Ø· Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ù†Ø§ÙˆÛŒÙ† ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ Ø´Ù…Ø§Ø±Ù‡ Ø¨Ø¯Ù‡ÛŒØ¯ØŒ Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† ØªÙˆØ¶ÛŒØ­ ÛŒØ§ Ù…Ù‚Ø¯Ù…Ù‡."
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
            )
            
            translation_text = response.choices[0].message.content.strip()
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ±Ø¬Ù…Ù‡â€ŒÙ‡Ø§
            translations = []
            for line in translation_text.split('\n'):
                line = line.strip()
                if line and any(line.startswith(f"{i}.") for i in range(1, 10)):
                    # Ø­Ø°Ù Ø´Ù…Ø§Ø±Ù‡ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ÛŒ Ø®Ø·
                    translations.append(line.split('.', 1)[1].strip())
            
            # Ø§Ø¹Ù…Ø§Ù„ ØªØ±Ø¬Ù…Ù‡ Ø¨Ù‡ Ø§Ø®Ø¨Ø§Ø±
            for j, item in enumerate(batch):
                if j < len(translations):
                    translated_title = translations[j]
                    item['title_fa'] = translated_title
                else:
                    item['title_fa'] = item['title']  # Ø§Ú¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù…ÙˆÙÙ‚ Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                
                translated_items.append(item)
        
        return translated_items
    except Exception as e:
        logger.error(f"Error translating news: {str(e)}")
        # If translation fails, return the original news
        for item in news_items:
            item['title_fa'] = item['title']
        return news_items


def get_crypto_news(limit: int = 10, translate: bool = True, include_canada: bool = True) -> List[Dict[str, Any]]:
    """
    Ø¯Ø±ÛŒØ§ÙØª Ùˆ ØªØ±Ú©ÛŒØ¨ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù
    
    Args:
        limit (int): ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
        translate (bool): Ø¢ÛŒØ§ Ø§Ø®Ø¨Ø§Ø± ØªØ±Ø¬Ù…Ù‡ Ø´ÙˆÙ†Ø¯
        include_canada (bool): Ø¢ÛŒØ§ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§Ø²Ø§Ø± Ú©Ø§Ù†Ø§Ø¯Ø§ Ø´Ø§Ù…Ù„ Ø´ÙˆØ¯
        
    Returns:
        List[Dict[str, Any]]: Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø± ØªØ±Ú©ÛŒØ¨ Ø´Ø¯Ù‡
    """
    # Ø§ÙˆÙ„ Ø³Ø¹ÛŒ Ú©Ù†ÛŒÙ… Ø§Ø² API Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
    if HAS_NEWS_API:
        try:
            logger.info("Attempting to get news from specialized API...")
            api_news = get_crypto_news_from_api(items_per_page=limit)
            
            if api_news:
                logger.info(f"{len(api_news)} news items received from specialized API")
                
                # Add Canadian news if needed
                if include_canada:
                    canada_news = get_canadian_crypto_news_from_api(items_per_page=limit // 2)
                    if canada_news:
                        logger.info(f"{len(canada_news)} Canadian news items received from specialized API")
                        # Ø§Ø¯ØºØ§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¹Ø¯Ù… ØªÚ©Ø±Ø§Ø±
                        seen_urls = {item['url'] for item in api_news}
                        for item in canada_news:
                            if item['url'] not in seen_urls:
                                api_news.append(item)
                                seen_urls.add(item['url'])
                
                # ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø§Ø³Øª
                if translate and OPENAI_API_KEY:
                    try:
                        api_news = translate_news(api_news)
                    except Exception as e:
                        logger.error(f"Error translating news: {str(e)}")
                        # If translation fails, fill the Persian title field with the English title
                        for item in api_news:
                            if 'title_fa' not in item:
                                item['title_fa'] = item['title']
                else:
                    # Ø§Ú¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù†ÛŒØ§Ø² Ù†ÛŒØ³ØªØŒ Ø¹Ù†ÙˆØ§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ú©Ù¾ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    for item in api_news:
                        item['title_fa'] = item['title']
                
                # ØªØ¨Ø¯ÛŒÙ„ timestamp Ø¨Ù‡ ØªØ§Ø±ÛŒØ® Ùˆ Ø²Ù…Ø§Ù† Ø®ÙˆØ§Ù†Ø§
                for item in api_news:
                    if 'published_at' in item and not item.get('published_date'):
                        try:
                            # ØªØ¨Ø¯ÛŒÙ„ ISO format Ø¨Ù‡ datetime
                            dt = datetime.fromisoformat(item['published_at'].replace('Z', '+00:00'))
                            dt_toronto = dt.astimezone(toronto_tz)
                            item['published_date'] = dt_toronto.strftime('%Y-%m-%d %H:%M')
                            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† published_on Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ…ÛŒ
                            if 'published_on' not in item:
                                item['published_on'] = int(dt.timestamp())
                        except Exception as dt_error:
                            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ®: {str(dt_error)}")
                            item['published_date'] = datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M')
                            item['published_on'] = int(time.time())
                
                # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø±
                api_news.sort(key=lambda x: x.get('published_on', 0), reverse=True)
                
                # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±
                return api_news[:limit]
        except Exception as api_err:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² API ØªØ®ØµØµÛŒ: {str(api_err)}")
            # Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ Ø±ÙˆØ´ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
    
    # Ø§Ú¯Ø± API Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¨Ø§Ø´Ø¯ ÛŒØ§ Ø®Ø·Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² Ø±ÙˆØ´ Ù‚Ø¯ÛŒÙ…ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    logger.warning("Using old sources to retrieve news")
    
    # ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù‡Ø± Ù…Ù†Ø¨Ø¹
    source_count = 4 if include_canada else 3
    per_source = limit // source_count + 1
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù
    cryptocompare_news = get_cryptocompare_news(limit=per_source)
    coindesk_news = get_coindesk_news(limit=per_source)
    cointelegraph_news = get_cointelegraph_news(limit=per_source)
    
    # ØªØ±Ú©ÛŒØ¨ Ø§Ø®Ø¨Ø§Ø±
    all_news = []
    all_news.extend(cryptocompare_news)
    all_news.extend(coindesk_news)
    all_news.extend(cointelegraph_news)
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø®Ø¨Ø§Ø± CMC Markets Canada
    if include_canada:
        try:
            # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ùˆ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CMC Markets Canada
            cmc_canada_content = get_combined_cmc_canada_content(max_news=per_source, max_analysis=2)
            
            # ØªØ¨Ø¯ÛŒÙ„ ÙØ±Ù…Øª Ø§Ø®Ø¨Ø§Ø± CMC Markets Canada Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
            for item in cmc_canada_content:
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
                if 'published_on' not in item:
                    item['published_on'] = int(time.time())
                
                # Ø§ÙØ²ÙˆØ¯Ù† Ù…Ù†Ø¨Ø¹ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
                if 'source' not in item:
                    item['source'] = 'CMC Markets Canada'
                
                # ØªØ¨Ø¯ÛŒÙ„ URL ØªØµÙˆÛŒØ±
                if 'image_url' in item and 'imageurl' not in item:
                    item['imageurl'] = item['image_url']
                
                # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
                if 'tags' not in item:
                    item['tags'] = ['canada', 'market']
                elif 'canada' not in item['tags']:
                    item['tags'].append('canada')
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù„ÛŒØ³Øª Ú©Ù„
                all_news.append(item)
            
            logger.info(f"Added {len(cmc_canada_content)} news items from CMC Markets Canada")
        except Exception as e:
            logger.error(f"Error fetching CMC Markets Canada content: {str(e)}")
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø§Ø¨ØªØ¯Ø§)
    all_news.sort(key=lambda x: x.get('published_on', 0), reverse=True)
    
    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±
    all_news = all_news[:limit]
    
    # ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    try:
        if translate and OPENAI_API_KEY:
            # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª APIØŒ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            max_translate_items = min(5, len(all_news))
            news_to_translate = all_news[:max_translate_items]
            
            logger.info(f"Translating {max_translate_items} news items out of {len(all_news)} total")
            translated_news = translate_news(news_to_translate)
            
            # Ø§Ø®Ø¨Ø§Ø±ÛŒ Ú©Ù‡ ØªØ±Ø¬Ù…Ù‡ Ù†Ø´Ø¯Ù†Ø¯ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            for i in range(max_translate_items, len(all_news)):
                all_news[i]['title_fa'] = all_news[i]['title']
                
            # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø§Ø®Ø¨Ø§Ø± ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯Ù‡
            all_news[:max_translate_items] = translated_news
        else:
            # Ø§Ú¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù†ÛŒØ§Ø² Ù†ÛŒØ³Øª ÛŒØ§ Ø§Ù…Ú©Ø§Ù† Ø¢Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
            for item in all_news:
                item['title_fa'] = item['title']
    except Exception as e:
        logger.error(f"Error in news translation in main function: {str(e)}")
        # In case of any error, all titles will be displayed in English
        for item in all_news:
            item['title_fa'] = item['title']
    
    # ØªØ¨Ø¯ÛŒÙ„ timestamp Ø¨Ù‡ ØªØ§Ø±ÛŒØ® Ùˆ Ø²Ù…Ø§Ù† Ø®ÙˆØ§Ù†Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø·Ù‚Ù‡ Ø²Ù…Ø§Ù†ÛŒ ØªÙˆØ±Ù†ØªÙˆ
    for item in all_news:
        if 'published_on' in item:
            dt_utc = datetime.utcfromtimestamp(item['published_on'])
            dt_toronto = dt_utc.replace(tzinfo=pytz.utc).astimezone(toronto_tz)
            item['published_date'] = dt_toronto.strftime('%Y-%m-%d %H:%M')
    
    return all_news


def get_crypto_sentiment_analysis() -> Dict[str, Any]:
    """
    Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø²Ù‡Ø§ÛŒ Ø¯ÛŒØ¬ÛŒØªØ§Ù„
    
    Returns:
        Dict[str, Any]: ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø±
    """
    # Ø§Ú¯Ø± Open AI API Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
    if not OPENAI_API_KEY:
        return {
            "overall_sentiment": "neutral",
            "sentiment_score": 50,
            "bitcoin_sentiment": "neutral",
            "ethereum_sentiment": "neutral",
            "updated_at": datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M'),
            "data_available": False
        }
    
    # Ø§ÙˆÙ„ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù†ØªØ§ÛŒØ¬ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒÙ… Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ù†Ø¨Ø§Ø´Ù†Ø¯
    # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ø­Ø§ÙØ¸Ù‡ Ù†Ù‡Ø§Ù† Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ API
    cached_sentiment_file = os.path.join(os.path.dirname(__file__), "cached_sentiment.json")
    try:
        if os.path.exists(cached_sentiment_file):
            with open(cached_sentiment_file, 'r') as f:
                cached_data = json.load(f)
                
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡ Ù†Ù‡Ø§Ù† (Ú©Ù…ØªØ± Ø§Ø² 30 Ø¯Ù‚ÛŒÙ‚Ù‡)
            cached_time = cached_data.get("cached_at", 0)
            if time.time() - cached_time < 1800:  # 30 Ø¯Ù‚ÛŒÙ‚Ù‡
                logger.info("Using cached sentiment analysis (less than 30 minutes old)")
                return cached_data
            else:
                logger.info("Cached sentiment analysis has expired (more than 30 minutes old)")
    except Exception as cache_err:
        logger.error(f"Error reading cached sentiment analysis: {str(cache_err)}")
    
    try:
        # Ø¯Ø±ÛŒØ§ÙØª Ú†Ù†Ø¯ Ø®Ø¨Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
        news = get_crypto_news(limit=5, translate=False)
        
        # Ø§Ú¯Ø± Ø®Ø¨Ø±ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯
        if not news:
            return {
                "overall_sentiment": "neutral",
                "sentiment_score": 50,
                "bitcoin_sentiment": "neutral",
                "ethereum_sentiment": "neutral",
                "updated_at": datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M'),
                "data_available": False
            }
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
        news_titles = "\n".join([f"- {item['title']}" for item in news])
        
        client = OpenAI(api_key=OPENAI_API_KEY)
        
        # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² f-string Ø¨Ø§ Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ
        prompt_first_part = "Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø²ÛŒØ±ØŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ø±Ù…Ø²Ø§Ø±Ø²Ù‡Ø§ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:\n\n"
        prompt_second_part = """

Ù„Ø·ÙØ§Ù‹ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© JSON Ø¨Ø§ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯:
{
  "overall_sentiment": "bullish/neutral/bearish",
  "sentiment_score": <Ø¹Ø¯Ø¯ Ø¨ÛŒÙ† 0 ØªØ§ 100. 0 Ú©Ø§Ù…Ù„Ø§Ù‹ bearishØŒ 50 neutralØŒ 100 Ú©Ø§Ù…Ù„Ø§Ù‹ bullish>,
  "bitcoin_sentiment": "bullish/neutral/bearish",
  "ethereum_sentiment": "bullish/neutral/bearish",
  "short_analysis": "<ÛŒÚ© ÛŒØ§ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ>"
}
"""
        prompt = prompt_first_part + news_titles + prompt_second_part
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.2,
        )
        
        result = json.loads(response.choices[0].message.content)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø²Ù…Ø§Ù† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
        result["updated_at"] = datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M')
        result["data_available"] = True
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ù†Ù‡Ø§Ù†
        try:
            result["cached_at"] = time.time()
            with open(cached_sentiment_file, 'w') as f:
                json.dump(result, f)
            logger.info("Sentiment analysis saved in cache")
        except Exception as cache_err:
            logger.error(f"Error saving sentiment analysis in cache: {str(cache_err)}")
        
        return result
    except Exception as e:
        logger.error(f"Error in market sentiment analysis: {str(e)}")
        
        # If there was an error, try to use the cached data even if it's expired
        try:
            if os.path.exists(cached_sentiment_file):
                with open(cached_sentiment_file, 'r') as f:
                    cached_data = json.load(f)
                logger.info("Using expired cached sentiment analysis due to error")
                return cached_data
        except Exception as cache_err:
            logger.error(f"Error reading expired cached sentiment analysis: {str(cache_err)}")
        
        # If the cache also had a problem, we return default data
        return {
            "overall_sentiment": "neutral",
            "sentiment_score": 50,
            "bitcoin_sentiment": "neutral",
            "ethereum_sentiment": "neutral",
            "updated_at": datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M'),
            "data_available": False,
            "error": str(e)
        }


def get_fear_greed_index() -> Dict[str, Any]:
    """
    Get Fear and Greed index of the market
    
    Returns:
        Dict[str, Any]: Fear and Greed index data
    """
    try:
        url = "https://api.alternative.me/fng/"
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if data.get("metadata", {}).get("error") is None:
                fng_data = data.get("data", [{}])[0]
                
                # Convert date to Toronto timezone
                time_until_update = fng_data.get("time_until_update", "")
                timestamp = int(fng_data.get("timestamp", "0"))
                dt_utc = datetime.utcfromtimestamp(timestamp)
                dt_toronto = dt_utc.replace(tzinfo=pytz.utc).astimezone(toronto_tz)
                
                return {
                    "value": int(fng_data.get("value", 50)),
                    "value_classification": fng_data.get("value_classification", "Neutral"),
                    "timestamp": timestamp,
                    "date": dt_toronto.strftime('%Y-%m-%d %H:%M'),
                    "time_until_update": time_until_update,
                    "data_available": True
                }
        
        # If data is not available or we encounter an error
        return {
            "value": 50,
            "value_classification": "Neutral",
            "timestamp": int(time.time()),
            "date": datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M'),
            "data_available": False
        }
    except Exception as e:
        logger.error(f"Error retrieving fear and greed index: {str(e)}")
        return {
            "value": 50,
            "value_classification": "Neutral",
            "timestamp": int(time.time()),
            "date": datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M'),
            "data_available": False,
            "error": str(e)
        }


def get_market_insights() -> Dict[str, Any]:
    """
    Get market insights and analysis for cryptocurrencies
    
    Returns:
        Dict[str, Any]: Market insights and analysis data
    """
    # Get various data
    news = get_crypto_news(limit=8, translate=True, include_canada=True)
    sentiment = get_crypto_sentiment_analysis()
    fear_greed = get_fear_greed_index()
    
    # Get CMC Markets Canada news separately
    cmc_canada_content = []
    try:
        cmc_canada_content = get_combined_cmc_canada_content(max_news=3, max_analysis=2)
        logger.info(f"Retrieved {len(cmc_canada_content)} items from CMC Markets Canada")
    except Exception as e:
        logger.error(f"Error getting CMC Markets Canada content: {str(e)}")
    
    # Combine and return the data
    return {
        "news": news,
        "sentiment": sentiment,
        "fear_greed_index": fear_greed,
        "cmc_canada": cmc_canada_content,
        "updated_at": datetime.now(toronto_tz).strftime('%Y-%m-%d %H:%M')
    }


def format_market_insights_for_telegram(insights: Dict[str, Any]) -> str:
    """
    Format market insights for sending to Telegram
    
    Args:
        insights (Dict[str, Any]): Market insights data
        
    Returns:
        str: Formatted text for Telegram
    """
    news = insights.get('news', [])
    sentiment = insights.get('sentiment', {})
    fear_greed = insights.get('fear_greed_index', {})
    cmc_canada = insights.get('cmc_canada', [])
    
    # Set emoji based on market sentiment
    sentiment_emoji = "ğŸ˜"  # neutral by default
    if sentiment.get('overall_sentiment') == 'bullish':
        sentiment_emoji = "ğŸš€"
    elif sentiment.get('overall_sentiment') == 'bearish':
        sentiment_emoji = "ğŸ»"
    
    # Set emoji for fear and greed index
    fng_value = fear_greed.get('value', 50)
    fng_emoji = "ğŸ˜"  # neutral by default
    if fng_value >= 75:
        fng_emoji = "ğŸ˜ˆ"  # extreme greed
    elif fng_value >= 55:
        fng_emoji = "ğŸ˜€"  # greed
    elif fng_value <= 25:
        fng_emoji = "ğŸ˜±"  # extreme fear
    elif fng_value <= 45:
        fng_emoji = "ğŸ˜¨"  # fear
    
    # Build the main text using an alternative method to avoid f-string issues with non-English characters
    message_parts = [
        "ğŸŒ *Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø²Ù‡Ø§ÛŒ Ø¯ÛŒØ¬ÛŒØªØ§Ù„* ğŸŒ",
        f"ØªØ§Ø±ÛŒØ®: {insights.get('updated_at')}",
        "",
        f"{sentiment_emoji} *ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø±:*",
        f"â€¢ ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ: {sentiment.get('overall_sentiment', 'Ù†Ø§Ù…Ø´Ø®Øµ')}",
        f"â€¢ Ø§Ù…ØªÛŒØ§Ø²: {sentiment.get('sentiment_score', 50)}",
        f"â€¢ Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ†: {sentiment.get('bitcoin_sentiment', 'Ù†Ø§Ù…Ø´Ø®Øµ')}",
        f"â€¢ Ø§ØªØ±ÛŒÙˆÙ…: {sentiment.get('ethereum_sentiment', 'Ù†Ø§Ù…Ø´Ø®Øµ')}",
        "",
        f"{fng_emoji} *Ø´Ø§Ø®Øµ ØªØ±Ø³ Ùˆ Ø·Ù…Ø¹:*",
        f"â€¢ Ø¹Ø¯Ø¯: {fear_greed.get('value', 'Ù†Ø§Ù…Ø´Ø®Øµ')}",
        f"â€¢ ÙˆØ¶Ø¹ÛŒØª: {fear_greed.get('value_classification', 'Ù†Ø§Ù…Ø´Ø®Øµ')}",
        "",
        "ğŸ“° *Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø±:*"
    ]
    message = "\n".join(message_parts)
    
    # Add news
    news_count = 0
    for i, item in enumerate(news[:4], 1):
        # Use the English title if available
        title = item.get('title_fa', item.get('title', ''))
        url = item.get('url', '')
        source = item.get('source', '')
        published_date = item.get('published_date', '')
        
        message += f"\n{i}. [{title}]({url})"
        message += f"\n   Ù…Ù†Ø¨Ø¹: {source} | {published_date}"
        news_count += 1
    
    # Add CMC Markets Canada news
    if cmc_canada:
        # If news was already added, create a line break
        if news_count > 0:
            message += "\n"
        
        message += "\nğŸ‡¨ğŸ‡¦ *ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø²Ø§Ø± Ú©Ø§Ù†Ø§Ø¯Ø§:*"
        
        for i, item in enumerate(cmc_canada[:2], news_count + 1):
            title = item.get('title_fa', item.get('title', ''))
            url = item.get('url', '')
            source = "CMC Markets Canada"
            content_type = item.get('content_type', 'news')
            
            # Select appropriate emoji for content type
            emoji = "ğŸ“Š" if content_type == 'analysis' else "ğŸ“°"
            
            message += f"\n{i}. {emoji} [{title}]({url})"
            message += f"\n   Ù…Ù†Ø¨Ø¹: {source}"
    
    # Add short analysis
    if sentiment.get('short_analysis'):
        message += f"\n\nğŸ” *ØªØ­Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡:*\n{sentiment.get('short_analysis')}"
    
    return message


def get_crypto_news_formatted_for_telegram() -> str:
    """
    Get cryptocurrency news formatted for Telegram
    
    Returns:
        str: Formatted text for Telegram
    """
    insights = get_market_insights()
    return format_market_insights_for_telegram(insights)


if __name__ == "__main__":
    # Module test
    print("Getting cryptocurrency news...")
    news = get_crypto_news(limit=5, translate=True)
    for item in news:
        print(f"Title: {item.get('title')}")
        print(f"Translation: {item.get('title_fa', '')}")
        print(f"Source: {item.get('source')}")
        print(f"Date: {item.get('published_date', '')}")
        print(f"Link: {item.get('url')}")
        print("-" * 50)
    
    print("\nMarket Sentiment Analysis:")
    sentiment = get_crypto_sentiment_analysis()
    print(json.dumps(sentiment, indent=2, ensure_ascii=False))
    
    print("\nFear and Greed Index:")
    fear_greed = get_fear_greed_index()
    print(json.dumps(fear_greed, indent=2, ensure_ascii=False))
    
    print("\nTelegram Report:")
    telegram_message = get_crypto_news_formatted_for_telegram()
    print(telegram_message)